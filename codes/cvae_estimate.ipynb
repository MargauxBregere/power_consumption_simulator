{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script allows to generate a N-sample daily power consumption profile,\n",
    "# for each of the data test days and for each of the nb_init decoders created with cvae_generate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "from cvae import CVAE, simulate_cvae\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the daily exogeous variables used to train CVAEs:  \n",
    "#       rescaled temperatures (3 temperatures computed using a dimensionallity reduction method - PCA)\n",
    "#       a smooth rescaled temperature \n",
    "#       position in the year (0 on January 1., 1 on December, 31.)\n",
    "#       working day or not \n",
    "#       tariff daily profile (48 * 2 = 96 variables : 48 with 1 if tariff is Low, 0 if not and 48 with 1 if tariff is High, 0 if not)\n",
    "\n",
    "cols_data = []\n",
    "cols_price = []\n",
    "cols_temp = ['temp_smooth_n']\n",
    "\n",
    "for i in range(48):\n",
    "    cols_data.append('consumption_{:02}'.format(i))\n",
    "    cols_price.append('low_{:02}'.format(i))\n",
    "    cols_price.append('high_{:02}'.format(i))\n",
    "\n",
    "for i in range(3):\n",
    "    cols_temp.append('pca_temp_{:1}'.format(i))\n",
    "cols_cond = cols_temp + ['pos_y', 'work_day'] + cols_price \n",
    "cond_dim = len(cols_cond) \n",
    "\n",
    "input_dim = 48\n",
    "latent_dim = 4\n",
    "nb_init = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create conditional vector to generate sample\n",
    "\n",
    "df_test = pd.read_feather('../data/test_ToU.feather')\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "dates = df_test['date']\n",
    "\n",
    "GMT = []\n",
    "for d in range(len(dates)):\n",
    "    for h in range(24):\n",
    "        GMT = GMT + [dates[d].replace(hour=h), dates[d].replace(hour=h,minute=30)]\n",
    "\n",
    "d0 = df_test.index[0]\n",
    "cond = tf.reshape(df_test.loc[d0, cols_cond], shape=[1, cond_dim])\n",
    "for d in df_test.index[1:len(df_test.index)]:\n",
    "    cond = tf.concat([cond, tf.reshape(df_test.loc[d, cols_cond], shape=[1, cond_dim])], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sim = 200\n",
    "\n",
    "for clustering_name in ['nmf']:\n",
    "    df_train = pd.read_feather('../data/train_' + clustering_name + '.feather')\n",
    "    df_train['date'] = pd.to_datetime(df_train['date'])     \n",
    "    \n",
    "    if (clustering_name!='Std') & (clustering_name!='ToU'):\n",
    "        nb_cluster = 4\n",
    "    else: \n",
    "        nb_cluster=1\n",
    "    for c in range(nb_cluster): \n",
    "        if (clustering_name!='Std') & (clustering_name!='ToU'):\n",
    "            df_train_c = df_train.loc[df_train['cluster'] == c]\n",
    "        else:\n",
    "            df_train_c = df_train\n",
    "            \n",
    "        conso_max = max(df_train_c.loc[:,cols_data].max())\n",
    "        conso_min = min(df_train_c.loc[:,cols_data].min())\n",
    "        \n",
    "        for it in range(nb_init):\n",
    "            random.seed(0)\n",
    "            if (clustering_name=='Std') | (clustering_name=='ToU'):\n",
    "                model = tf.keras.models.load_model('../data/decoders/' + clustering_name + '_' + str(it) + '.h5', compile=False)\n",
    "            else:\n",
    "                model = tf.keras.models.load_model('../data/decoders/' + clustering_name +'cluster_' + str(c) + '_' + str(it) + '.h5', compile=False)\n",
    "            eps = tf.random.normal(shape=[len(dates),latent_dim], dtype=tf.dtypes.float64)\n",
    "            conso = simulate_cvae(model, GMT, eps, cond, conso_min, conso_max)\n",
    "            for _ in range (nb_sim-1):\n",
    "                eps = tf.random.normal(shape=[len(dates),latent_dim], dtype=tf.dtypes.float64)\n",
    "                conso = pd.concat([conso,simulate_cvae(model, GMT, eps, cond, conso_min, conso_max)])\n",
    "            if (clustering_name=='Std') | (clustering_name=='ToU'):\n",
    "                conso.reset_index(drop=True).to_feather('../data/estimations/cvae_' + clustering_name + '_' + str(it) + '.feather')\n",
    "            else:\n",
    "                conso.reset_index(drop=True).to_feather('../data/estimations/cvae_' + clustering_name + '_cluster_' + str(c) + '_' + str(it) + '.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

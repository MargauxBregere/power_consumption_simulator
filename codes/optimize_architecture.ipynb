{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from cvae import CVAE\n",
    "import random\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "random.seed(0)\n",
    "HP_CLUSTER = hp.HParam('cluster', hp.Discrete([0,1,2,3]))\n",
    "HP_LATENT_DIM = hp.HParam('latent_dim',    hp.Discrete([3,4,5,10,20]))\n",
    "#HP_LEARNING_RATE = hp.HParam('learning rate', hp.Discrete([0.01,0.001]))\n",
    "#HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','linear', 'sigmoid'])) \n",
    "HP_UNITS = hp.HParam('units', hp.Discrete([10, 15, 20, 25]))\n",
    "#HP_LAYERS = hp.HParam('layers', hp.Discrete([1, 2, 3]))\n",
    "#HP_NORM = hp.HParam('norm', hp.Discrete(['L1', 'L2']))\n",
    "HP_MOVING_KL_COEFF = hp.HParam('moving_kl', hp.Discrete(['constant']))\n",
    "#HP_MOVING_KL_COEFF = hp.HParam('moving_kl', hp.Discrete(['linear', 'sigmoid', 'constant']))\n",
    "HP_KL_COEFF = hp.HParam('kl_coeff', hp.Discrete([10]))\n",
    "#HP_KL_COEFF = hp.HParam('kl_coeff', hp.Discrete([0.01, 0.1, 0.5, 1, 5, 10, 100]))\n",
    "\n",
    "MSE = 'mse'\n",
    "KL = 'kl'\n",
    "LOSS = 'loss'\n",
    "MSE_test = 'mse_test'\n",
    "KL_test = 'kl_test'\n",
    "LOSS_test = 'loss_test'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/latent_dim_layers').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_CLUSTER, HP_LATENT_DIM, HP_UNITS, HP_MOVING_KL_COEFF, HP_KL_COEFF], \n",
    "        #hparams=[HP_CLUSTER, HP_LATENT_DIM,HP_LEARNING_RATE, HP_ACTIVATION, HP_UNITS,\n",
    "        #         HP_LAYERS, HP_NORM, HP_MOVING_KL_COEFF, HP_KL_COEFF],\n",
    "        metrics=[hp.Metric(MSE, display_name='Mean Square Error'),\n",
    "                 hp.Metric(KL, display_name='Kullbach Leibler divergence'),\n",
    "                 hp.Metric(LOSS, display_name='Loss'),\n",
    "                 hp.Metric(MSE_test, display_name='Mean Square Error Test'),\n",
    "                 hp.Metric(KL_test, display_name='Kullbach Leibler divergence Test'),\n",
    "                 hp.Metric(LOSS_test, display_name='Loss Test')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "activation = 'relu'\n",
    "clustering_name = 'nmf'\n",
    "kl_coeff = 10\n",
    "moving_kl_coeff = 'constant'\n",
    "\n",
    "df_train = pd.read_feather('../data/train_' + clustering_name + '.feather')\n",
    "df_test = pd.read_feather('../data/test_' + clustering_name + '.feather')\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "clusters = pd.read_feather('../data/sum_up_' + clustering_name + '.feather')\n",
    "nb_clust = clusters.cluster.size\n",
    "clusters = clusters.set_index('cluster')\n",
    "\n",
    "epochs = 50\n",
    "input_dim = 48\n",
    "nb_dates_for_train = len(df_train['date'].unique())\n",
    "nb_dates_for_test = len(df_test['date'].unique())  # - nb_dates_for_train\n",
    "\n",
    "nb_layers_encoder = 2\n",
    "nb_layers_decoder = 2\n",
    "weights_decoder = None\n",
    "weights_encoder = None\n",
    "\n",
    "cols_clust = []\n",
    "for c in clusters.index:\n",
    "    df_train['cluster_{:02}'.format(c)] = np.array(1 * (df_train['cluster'] == c), dtype='float64')\n",
    "    df_test['cluster_{:02}'.format(c)] = np.array(1 * (df_test['cluster'] == c), dtype='float64')\n",
    "    cols_clust.append('cluster_{:02}'.format(c))\n",
    "\n",
    "cols_data = []\n",
    "cols_price = []\n",
    "cols_temp = ['temp_smooth_n']\n",
    "\n",
    "for i in range(48):\n",
    "    cols_data.append('consumption_{:02}'.format(i))\n",
    "    cols_price.append('low_{:02}'.format(i))\n",
    "    cols_price.append('high_{:02}'.format(i))\n",
    "\n",
    "for i in range(3):\n",
    "    cols_temp.append('pca_temp_{:1}'.format(i))\n",
    "cols_cond = cols_temp + ['pos_y', 'work_day'] + cols_price \n",
    "cond_dim = len(cols_cond) \n",
    "\n",
    "initializer = tf.keras.initializers.glorot_uniform(seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hparams):\n",
    "    \n",
    "    units_layers_encoder = [units, 2*latent_dim]\n",
    "    units_layers_decoder = [units, input_dim]\n",
    "    \n",
    "    train_mse = tf.keras.metrics.Mean('train_mse', dtype=tf.float64)\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float64)\n",
    "    train_kl = tf.keras.metrics.Mean('train_kl', dtype=tf.float64)\n",
    "    test_mse = tf.keras.metrics.Mean('test_mse', dtype=tf.float64)\n",
    "    test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float64)\n",
    "    test_kl = tf.keras.metrics.Mean('test_kl', dtype=tf.float64)\n",
    "\n",
    "    df_train_c = df_train.loc[df_train['cluster_{:02}'.format(c)] == 1]\n",
    "    df_test_c = df_test.loc[df_test['cluster_{:02}'.format(c)] == 1]\n",
    "\n",
    "    conso_max = max(df_train_c.loc[:, cols_data].max())\n",
    "    conso_min = min(df_train_c.loc[:, cols_data].min())\n",
    "    df_train_c.loc[:, cols_data] = (df_train_c.loc[:, cols_data] - conso_min) / (conso_max - conso_min)\n",
    "    df_test_c.loc[:, cols_data] = (df_test_c.loc[:, cols_data] - conso_min) / (conso_max - conso_min)\n",
    "\n",
    "    d0 = df_train_c.index[0]\n",
    "    df_train_tf = tf.reshape(df_train_c.loc[d0, cols_data], shape=[1, 48])\n",
    "    cond_train_tf = tf.reshape(df_train_c.loc[d0, cols_cond], shape=[1, cond_dim])\n",
    "    for d in df_train_c.index[1:len(df_train_c.index)]:\n",
    "        df_train_tf = tf.concat([df_train_tf, tf.reshape(df_train_c.loc[d, cols_data], shape=[1, 48])], 0)\n",
    "        cond_train_tf = tf.concat([cond_train_tf, tf.reshape(df_train_c.loc[d, cols_cond], shape=[1, cond_dim])], 0)\n",
    "\n",
    "    d0 = df_test_c.index[0]\n",
    "    df_test_tf = tf.reshape(df_test_c.loc[d0, cols_data], shape=[1, 48])\n",
    "    cond_test_tf = tf.reshape(df_test_c.loc[d0, cols_cond], shape=[1, cond_dim])\n",
    "    for d in df_test_c.index[1:len(df_test_c.index)]:\n",
    "        df_test_tf = tf.concat([df_test_tf, tf.reshape(df_test_c.loc[d, cols_data], shape=[1, 48])], 0)\n",
    "        cond_test_tf = tf.concat([cond_test_tf, tf.reshape(df_test_c.loc[d, cols_cond], shape=[1, cond_dim])], 0)\n",
    "\n",
    "    name = 'cluster' + str(c) + '_' + 'latentdim_' + str(latent_dim) + '_units_' + str(units) \n",
    "        \n",
    "    model = CVAE(input_dim, latent_dim, cond_dim,\n",
    "                 full_covariance_matrix=False,\n",
    "                 nb_layers_encoder=nb_layers_encoder,\n",
    "                 units_layers_encoder=units_layers_encoder,\n",
    "                 nb_layers_decoder=nb_layers_decoder,\n",
    "                 units_layers_decoder=units_layers_decoder,\n",
    "                 initializer_encoder=initializer,\n",
    "                 weights_encoder=weights_encoder,\n",
    "                 initializer_decoder=initializer,\n",
    "                 weights_decoder=weights_decoder,\n",
    "                 activation=activation,\n",
    "                 optimizer=optimizer,\n",
    "                 loss='L2')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if moving_kl == 'linear':\n",
    "            eta = (epoch / epochs) * kl_coeff\n",
    "        if moving_kl == 'sigmoid':\n",
    "            x = -6 + 12 * epoch / epochs\n",
    "            eta = (1 / (1 + np.math.exp(-x))) * kl_coeff\n",
    "        if moving_kl == 'constant':\n",
    "            eta = 1 * kl_coeff\n",
    "        model.train_step(df_train_tf, cond_train_tf, eta)\n",
    "\n",
    "    for d in range(df_train_tf.shape[0]):\n",
    "        x = tf.reshape(df_train_tf[d,].numpy(), shape=[1, model.input_dim])\n",
    "        cond = tf.reshape(cond_train_tf[d,].numpy(), shape=[1, model.cond_dim])\n",
    "        train_mse(model.compute_mse(x, cond))\n",
    "        train_loss(model.compute_loss(x, cond))\n",
    "        train_kl(model.compute_kl_err(x, cond))\n",
    "\n",
    "    for d_t in range(df_test_tf.shape[0]):\n",
    "        x = tf.reshape(df_test_tf[d_t,].numpy(), shape=[1, input_dim])\n",
    "        cond = tf.reshape(cond_test_tf[d_t,].numpy(), shape=[1, cond_dim])\n",
    "        test_mse(model.compute_mse(x, cond))\n",
    "        test_loss(model.compute_loss(x, cond))\n",
    "        test_kl(model.compute_kl_err(x, cond))\n",
    "\n",
    "    #model.decoder_net.save('../data/decoders/hyp_opt/' + name + '.h5')\n",
    "\n",
    "    return train_mse.result(), train_loss.result(), train_kl.result(), test_mse.result(), test_loss.result(), test_kl.result()\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        train_mse, train_loss, train_kl, test_mse, test_loss, test_kl = train_model(hparams)\n",
    "        tf.summary.scalar(MSE, train_mse, step=1)\n",
    "        tf.summary.scalar(LOSS, train_loss, step=1)\n",
    "        tf.summary.scalar(KL, train_kl, step=1)\n",
    "        tf.summary.scalar(MSE_test, test_mse, step=1)\n",
    "        tf.summary.scalar(LOSS_test, test_loss, step=1)\n",
    "        tf.summary.scalar(KL_test, test_kl, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for c in HP_CLUSTER.domain.values:\n",
    "    for latent_dim in HP_LATENT_DIM.domain.values:\n",
    "        for moving_kl in HP_MOVING_KL_COEFF.domain.values:\n",
    "            for kl_coeff in HP_KL_COEFF.domain.values:\n",
    "    #            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "    #                for activation in HP_ACTIVATION.domain.values:\n",
    "    #                    for norm in HP_NORM.domain.values:\n",
    "    #                        for layers in HP_LAYERS.domain.values:\n",
    "    #                             if layers >1:\n",
    "                for units in HP_UNITS.domain.values:\n",
    "                    hparams = {HP_CLUSTER: c,\n",
    "                        HP_LATENT_DIM: latent_dim,\n",
    "   #                    HP_LEARNING_RATE: learning_rate,\n",
    "   #                    HP_ACTIVATION: activation,\n",
    "                        HP_UNITS: units,\n",
    "   #                    HP_LAYERS: layers,\n",
    "                        HP_MOVING_KL_COEFF: moving_kl,\n",
    "                        HP_KL_COEFF: kl_coeff,\n",
    "   #                    HP_NORM: norm,\n",
    "                        }\n",
    "    #                             else:\n",
    "    #                                hparams = {HP_CLUSTER: c,\n",
    "    #                                    HP_LATENT_DIM: latent_dim,\n",
    "    #                                    HP_LEARNING_RATE: learning_rate,\n",
    "    #                                    HP_ACTIVATION: activation,\n",
    "    #                                    HP_UNITS: 0,\n",
    "    #                                    HP_LAYERS: layers,\n",
    "    #                                    HP_MOVING_KL_COEFF: moving_kl,\n",
    "    #                                    HP_KL_COEFF: kl_coeff,\n",
    "    #                                    HP_NORM: norm,\n",
    "    #                                    }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('logs/hp_architecture/' + run_name, hparams)\n",
    "            session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
